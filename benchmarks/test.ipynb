{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "AffineQuantizedTensor.from_hp_to_intx_static() missing 1 required positional argument: 'target_dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m block_size \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     30\u001b[0m target_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mint8\n\u001b[0;32m---> 31\u001b[0m qa \u001b[38;5;241m=\u001b[39m \u001b[43mAffineQuantizedTensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_hp_to_intx_static\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMappingType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mASYMMETRIC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_min\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m block_size \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     33\u001b[0m qb \u001b[38;5;241m=\u001b[39m AffineQuantizedTensor\u001b[38;5;241m.\u001b[39mfrom_hp_to_intx_static(B, MappingType\u001b[38;5;241m.\u001b[39mASYMMETRIC, block_size, target_dtype)\n",
      "\u001b[0;31mTypeError\u001b[0m: AffineQuantizedTensor.from_hp_to_intx_static() missing 1 required positional argument: 'target_dtype'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "from torchao.dtypes.affine_quantized_tensor import AffineQuantizedTensor\n",
    "import torch\n",
    "from torchao.quantization.quant_primitives import MappingType\n",
    "from torchao.ops import rowwise_scaled_linear_cutlass_s8s4\n",
    "\n",
    "A = torch.Tensor([\n",
    "    [1.0, 2.0, 3.0],\n",
    "    [4.0, 5.0, 6.0],\n",
    "    [7.0, 8.0, 9.0]]\n",
    ")\n",
    "\n",
    "B = torch.Tensor([\n",
    "    [-2.0, 1.0],\n",
    "    [3.0, -4.0],\n",
    "    [6.0, -8.0]]\n",
    ")\n",
    "\n",
    "C = torch.Tensor(\n",
    "    [20.0, -18.0]\n",
    ")\n",
    "\n",
    "block_size = [1, 0]\n",
    "target_dtype = torch.int8\n",
    "qa = AffineQuantizedTensor.from_hp_to_intx(A, MappingType.ASYMMETRIC, block_size, target_dtype, quant_min=-8, quant_max=7)\n",
    "block_size = [0, 1]\n",
    "qb = AffineQuantizedTensor.from_hp_to_intx(B, MappingType.ASYMMETRIC, block_size, target_dtype)\n",
    "\n",
    "print(qa)\n",
    "print(qb)\n",
    "\n",
    "print(rowwise_scaled_linear_cutlass_s8s4(qa.tensor_impl.data, qa.tensor_impl.scale, qb.tensor_impl.data, qb.tensor_impl.scale, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "CompilationError",
     "evalue": "at 1:0:\ndef fp8_blockwise_quant_weight_kernel(x_ptr, y_ptr, s_ptr, M, N, BLOCK_SIZE: tl.constexpr):\n^\nValueError(\"type fp8e4nv not supported in this architecture. The supported fp8 dtypes are ('fp8e4b15', 'fp8e5')\")",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCompilationError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 15\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRelative Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m error \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.05\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuant-Dequant error too high!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 15\u001b[0m \u001b[43mtest_quant_dequant\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[36], line 6\u001b[0m, in \u001b[0;36mtest_quant_dequant\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      5\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m qx, s \u001b[38;5;241m=\u001b[39m \u001b[43mfp8_blockwise_weight_quant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m x_reconstructed \u001b[38;5;241m=\u001b[39m fp8_blockwise_weight_dequant(qx, s, block_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[1;32m      9\u001b[0m error \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnorm(x \u001b[38;5;241m-\u001b[39m x_reconstructed) \u001b[38;5;241m/\u001b[39m torch\u001b[38;5;241m.\u001b[39mnorm(x)\n",
      "File \u001b[0;32m~/Documents/ao/torchao/prototype/blockwise_fp8/blockwise_quantization.py:78\u001b[0m, in \u001b[0;36mfp8_blockwise_weight_quant\u001b[0;34m(x, block_size)\u001b[0m\n\u001b[1;32m     76\u001b[0m s \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mnew_empty(M \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m block_size, N \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m block_size, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     77\u001b[0m grid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m meta: (triton\u001b[38;5;241m.\u001b[39mcdiv(M, meta[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBLOCK_SIZE\u001b[39m\u001b[38;5;124m'\u001b[39m]), triton\u001b[38;5;241m.\u001b[39mcdiv(N, meta[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBLOCK_SIZE\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m---> 78\u001b[0m \u001b[43mfp8_blockwise_quant_weight_kernel\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBLOCK_SIZE\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y, s\n",
      "File \u001b[0;32m~/Documents/ao/venv/lib/python3.12/site-packages/triton/runtime/jit.py:330\u001b[0m, in \u001b[0;36mKernelInterface.__getitem__.<locals>.<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, grid) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    325\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;124;03m    A JIT function is launched with: fn[grid](*args, **kwargs).\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;124;03m    Hence JITFunction.__getitem__ returns a callable proxy that\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;124;03m    memorizes the grid.\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/ao/venv/lib/python3.12/site-packages/triton/runtime/jit.py:623\u001b[0m, in \u001b[0;36mJITFunction.run\u001b[0;34m(self, grid, warmup, *args, **kwargs)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;66;03m# compile the kernel\u001b[39;00m\n\u001b[1;32m    622\u001b[0m src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mASTSource(\u001b[38;5;28mself\u001b[39m, signature, constants, configs[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 623\u001b[0m kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m    \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__dict__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache[device][key] \u001b[38;5;241m=\u001b[39m kernel\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_hook(key, signature, device, constants, options, configs, warmup, before\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/ao/venv/lib/python3.12/site-packages/triton/compiler/compiler.py:273\u001b[0m, in \u001b[0;36mcompile\u001b[0;34m(src, target, options)\u001b[0m\n\u001b[1;32m    271\u001b[0m module_map \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mget_module_map()\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 273\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_ir\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcodegen_fns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    275\u001b[0m     filter_traceback(e)\n",
      "File \u001b[0;32m~/Documents/ao/venv/lib/python3.12/site-packages/triton/compiler/compiler.py:100\u001b[0m, in \u001b[0;36mASTSource.make_ir\u001b[0;34m(self, options, codegen_fns, module_map, context)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmake_ir\u001b[39m(\u001b[38;5;28mself\u001b[39m, options, codegen_fns, module_map, context):\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mast_to_ttir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcodegen_fns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcodegen_fns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mmodule_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodule_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mCompilationError\u001b[0m: at 1:0:\ndef fp8_blockwise_quant_weight_kernel(x_ptr, y_ptr, s_ptr, M, N, BLOCK_SIZE: tl.constexpr):\n^\nValueError(\"type fp8e4nv not supported in this architecture. The supported fp8 dtypes are ('fp8e4b15', 'fp8e5')\")"
     ]
    }
   ],
   "source": [
    "from torchao.prototype.blockwise_fp8.blockwise_quantization import fp8_blockwise_weight_quant, fp8_blockwise_weight_dequant\n",
    "from torchao.prototype.blockwise_fp8.blockwise_fp8_gemm_triton import blockwise_fp8_gemm\n",
    "\n",
    "def test_quant_dequant():\n",
    "    torch.manual_seed(0)\n",
    "    x = torch.randn(256, 256)\n",
    "    qx, s = fp8_blockwise_weight_quant(x, block_size=128)\n",
    "    x_reconstructed = fp8_blockwise_weight_dequant(qx, s, block_size=128)\n",
    "\n",
    "    error = torch.norm(x - x_reconstructed) / torch.norm(x)\n",
    "    print(f\"Relative Error: {error.item():.6f}\")\n",
    "\n",
    "    assert error < 0.05, \"Quant-Dequant error too high!\"\n",
    "\n",
    "def test_blockwise_fp8_gemm():\n",
    "    torch.manual_seed(0)\n",
    "    M, N, K = 256, 256, 256\n",
    "    A = torch.randn(M, K)\n",
    "    B = torch.randn(K, N)\n",
    "\n",
    "    C = A @ B\n",
    "\n",
    "    A_q, A_s = fp8_blockwise_weight_quant(A, block_size=128)\n",
    "    B_q, B_s = fp8_blockwise_weight_quant(B, block_size=128)\n",
    "\n",
    "    C_q = blockwise_fp8_gemm(A_q, A_s, B_q, B_s)\n",
    "\n",
    "    error = torch.norm(C - C_q) / torch.norm(C)\n",
    "    print(f\"Relative Error: {error.item():.6f}\")\n",
    "\n",
    "    assert error < 0.05, \"Quantized GEMM error is too high!\"\n",
    "\n",
    "\n",
    "test_quant_dequant()\n",
    "test_blockwise_fp8_gemm()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
